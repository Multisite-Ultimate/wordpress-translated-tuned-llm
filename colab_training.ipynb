{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WordPress Translation Fine-Tuning (Mistral 7B)\n",
        "\n",
        "Fine-tune Mistral 7B for Englishâ†’Dutch WordPress translation using QLoRA on Google Colab Free Tier.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with GPU runtime (T4 is fine)\n",
        "- Upload your `train_subset.jsonl` file\n",
        "\n",
        "**Estimated time:** 2-4 hours for 10k examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (pinned versions for compatibility)\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers==4.44.0 datasets>=2.17.0 accelerate>=0.27.0\n",
        "!pip install -q peft>=0.8.2 trl==0.9.6 bitsandbytes>=0.42.0\n",
        "!pip install -q scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Upload Dataset\n",
        "\n",
        "Run this cell and upload the `train_subset.jsonl` file generated by the export script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Upload the dataset file\n",
        "print(\"Upload train_subset.jsonl (generated by export_for_colab.py)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "DATASET_FILE = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded: {DATASET_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('json', data_files=DATASET_FILE, split='train')\n",
        "print(f\"Dataset size: {len(dataset)} examples\")\n",
        "print(f\"Columns: {dataset.column_names}\")\n",
        "print(f\"\\nSample example:\\n{dataset[0]['text'][:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Model with 4-bit Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# 4-bit quantization config (essential for free Colab)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Loading model with 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"Model loaded! Memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=32,  # Reduced from 64 for memory\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# SFTConfig combines TrainingArguments with SFT-specific settings\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./wp-translation-adapter\",\n",
        "    num_train_epochs=1,  # Start with 1 epoch for testing\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,  # Effective batch size = 8\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.001,\n",
        "    max_grad_norm=0.3,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=25,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,  # Use FP16 for T4\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",  # Disable wandb etc.\n",
        "    seed=42,\n",
        "    group_by_length=True,\n",
        "    # SFT-specific settings\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"Training configuration ready!\")\n",
        "print(f\"- Epochs: {sft_config.num_train_epochs}\")\n",
        "print(f\"- Batch size: {sft_config.per_device_train_batch_size}\")\n",
        "print(f\"- Gradient accumulation: {sft_config.gradient_accumulation_steps}\")\n",
        "print(f\"- Effective batch size: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer (new API uses 'processing_class' instead of 'tokenizer')\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(f\"\\nStarting training... This may take 2-4 hours.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train!\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Training complete!\")\n",
        "print(f\"Time elapsed: {elapsed/3600:.2f} hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Adapter Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save adapter\n",
        "ADAPTER_PATH = \"./wp-translation-adapter-final\"\n",
        "\n",
        "model.save_pretrained(ADAPTER_PATH)\n",
        "tokenizer.save_pretrained(ADAPTER_PATH)\n",
        "\n",
        "print(f\"Adapter saved to {ADAPTER_PATH}\")\n",
        "!ls -la {ADAPTER_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download adapter to your local machine\n",
        "import shutil\n",
        "\n",
        "# Create zip file\n",
        "shutil.make_archive(\"wp-translation-adapter\", \"zip\", ADAPTER_PATH)\n",
        "\n",
        "# Download\n",
        "files.download(\"wp-translation-adapter.zip\")\n",
        "print(\"\\nDownload started! Save this file to use the adapter locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test translation\n",
        "def translate(text, source_lang=\"English\", target_lang=\"Dutch\"):\n",
        "    prompt = f\"\"\"<s>[INST] Translate the following WordPress text from {source_lang} to {target_lang}. Preserve any placeholders like %s, %d, or {{name}}.\n",
        "\n",
        "{text} [/INST]\"\"\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the translation (after [/INST])\n",
        "    if \"[/INST]\" in response:\n",
        "        response = response.split(\"[/INST]\")[-1].strip()\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with some WordPress strings\n",
        "test_strings = [\n",
        "    \"Add to cart\",\n",
        "    \"Your order has been placed successfully.\",\n",
        "    \"Please enter a valid email address.\",\n",
        "    \"%d items in your cart\",\n",
        "    \"Hello, {name}! Welcome back.\",\n",
        "]\n",
        "\n",
        "print(\"Translation Tests:\")\n",
        "print(\"=\" * 60)\n",
        "for text in test_strings:\n",
        "    translation = translate(text)\n",
        "    print(f\"EN: {text}\")\n",
        "    print(f\"NL: {translation}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Download the adapter zip file** - it contains the LoRA weights\n",
        "2. **Copy to your local project**: `models/adapters/nl/`\n",
        "3. **Run evaluation locally**: `./run.py evaluate nl`\n",
        "4. **For more training**: Increase `num_train_epochs` or use more data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
